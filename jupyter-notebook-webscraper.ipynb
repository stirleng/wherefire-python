{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "#constants\n",
    "#=========\n",
    "\n",
    "#tbody tag only occurs once on page (tbody is short for table body)\n",
    "RESULT_TABLE_TAG = 'tbody'\n",
    "\n",
    "#link tags (link to specific fire incident page)\n",
    "FIRE_NAMES_TAG = 'th'\n",
    "\n",
    "#specify certain request headers so the website doesn't 403 our request and we receive an xml response\n",
    "headers = {  'Connection': 'close', 'Accept': 'application/xml', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/111.0'} \n",
    "\n",
    "#other constants\n",
    "CURRENT_YEAR_URL = 'https://www.fire.ca.gov/incidents'\n",
    "PREV_YEAR_URL = 'https://www.fire.ca.gov/incidents/2022/'\n",
    "INCIDENT_URL_BASE = PREV_YEAR_URL #TODO:: change to CURRENT_YEAR_URL when done testing\n",
    "LAT_LONG_TEXT = 'Latitude / Longitude'\n",
    "CSV_COLUMN_NAMES = ['name','county','date_started','latitude','longitude']\n",
    "\n",
    "#/constants\n",
    "#==========\n",
    "\n",
    "def update_data_file():\n",
    "    #get webpage xml from response\n",
    "    with requests.Session() as s:\n",
    "        response = s.get(PREV_YEAR_URL, headers=headers)\n",
    "        response_xml = response.text\n",
    "    #parse xml using lxml parser\n",
    "    soup = BS(response_xml, 'lxml')\n",
    "\n",
    "    #find data table in page\n",
    "    data_table = soup.find(RESULT_TABLE_TAG)\n",
    "\n",
    "    #find all fire name elements (hypertext)\n",
    "    fire_name_elements = data_table.find_all(FIRE_NAMES_TAG)\n",
    "\n",
    "    #create lists for each fire attribute\n",
    "    fire_names = []\n",
    "    fire_counties = []\n",
    "    fire_start_dates = []\n",
    "    fire_latitudes = []\n",
    "    fire_longitudes = []\n",
    "    incident_urls = []\n",
    "    #iterate through table elements and store text\n",
    "    for fire_name_element in fire_name_elements:\n",
    "        fire_name_text = fire_name_element.contents[1].text.strip()\n",
    "        print(fire_name_text)\n",
    "        fire_county_name_element = fire_name_element.next_sibling.next_sibling #idk why it needs an extra .next_sibling but it does\n",
    "        fire_date_started_element = fire_county_name_element.next_sibling.next_sibling\n",
    "        fire_names.append(fire_name_text) #.child because fire names have an extra parent wrapper\n",
    "        fire_counties.append(fire_county_name_element.text.strip())\n",
    "        fire_start_dates.append(fire_date_started_element.text.strip())\n",
    "        split_date = fire_date_started_element.text.split('/')\n",
    "        #TODO:: use a regex to remove all alphanumeric characters in the fire name (to use in url - maybe only replace chars not permitted in urls?)\n",
    "        stripped_date = split_date[0].lstrip('0') + \"/\" + split_date[1].lstrip('0') + \"/\" + fire_name_text.replace(\"(\",\"\").replace(\")\",\"\").replace(\" \", \"-\")\n",
    "        incident_url = INCIDENT_URL_BASE + stripped_date\n",
    "        incident_urls.append(incident_url)\n",
    "        #TODO:: use forking to make the following faster\n",
    "        with requests.Session() as s:\n",
    "            response = s.get(incident_url, headers=headers)\n",
    "            response_xml = response.text\n",
    "            soup = BS(response_xml, 'lxml')\n",
    "            lat_long_title_element = soup.find(string=LAT_LONG_TEXT)\n",
    "            lat_long_element = lat_long_title_element.parent.next_sibling.next_sibling\n",
    "            #isolate latitude and longitude and convert from string to float\n",
    "            lat_long = lat_long_element.text.replace(\"[\", \"\").replace(\"]\", \"\").replace(\" \", \"\").split(\",\")\n",
    "            for val in lat_long:\n",
    "                float(val)\n",
    "            fire_latitudes.append(lat_long[0])\n",
    "            fire_longitudes.append(lat_long[1])\n",
    "\n",
    "\n",
    "    #find which fires are new\n",
    "    new_incidents_available = False\n",
    "    new_incidents_fire_names = []\n",
    "    df = pd.read_csv('fire_data.csv')\n",
    "    search_column_name = CSV_COLUMN_NAMES[0]\n",
    "    new_fire_data = {key: [] for key in CSV_COLUMN_NAMES} #create a dict of new data to append to csv\n",
    "    for fire_name, county_name, date_started,latitude,longitude in zip(fire_names, fire_counties, fire_start_dates,fire_latitudes,fire_longitudes):\n",
    "        new_fire = df[df[search_column_name] == fire_name]\n",
    "        if new_fire.empty:\n",
    "            new_incidents_available = True\n",
    "            new_incidents_fire_names.append(fire_name)\n",
    "            new_fire_data[CSV_COLUMN_NAMES[0]].append(fire_name)\n",
    "            new_fire_data[CSV_COLUMN_NAMES[1]].append(county_name)\n",
    "            new_fire_data[CSV_COLUMN_NAMES[2]].append(date_started)\n",
    "            new_fire_data[CSV_COLUMN_NAMES[3]].append(latitude)\n",
    "            new_fire_data[CSV_COLUMN_NAMES[4]].append(longitude)\n",
    "    new_fire_df = pd.DataFrame.from_dict(new_fire_data)\n",
    "    new_fire_df.to_csv('fire_data.csv', mode='a', header=False, index=False)\n",
    "\n",
    "    #note: returns a tuple of two vars\n",
    "    return new_incidents_available, new_incidents_fire_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "#api_secrets.py\n",
    "import api_secrets\n",
    "\n",
    "#constants\n",
    "#=========\n",
    "\n",
    "MAX_TWEET_LENGTH = 280\n",
    "\n",
    "#\\constants\n",
    "#==========\n",
    "\n",
    "def tweet(tweet_text_list):\n",
    "    # creates the tweepy Client object\n",
    "    client = tweepy.Client(consumer_key=api_secrets.consumer_key, consumer_secret=api_secrets.consumer_secret, access_token=api_secrets.access_token, access_token_secret=api_secrets.access_token_secret) \n",
    "    \n",
    "    # tweets the fire names\n",
    "    for tweet_text in tweet_text_list:  #need tweet list since may be multiple new fires between checks\n",
    "        client.create_tweet(text=tweet_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pleasant Fire\n",
      "Howard Fire\n",
      "97 Fire\n",
      "Manzanita Fire\n",
      "Irie Fire\n",
      "Garden Fire\n",
      "Dutch Fire\n",
      "Forward Fire\n",
      "Eliza Fire\n",
      "Barnes Fire\n",
      "Fork Fire\n",
      "Coyote Fire\n",
      "Mosquito Fire\n",
      "Power Fire\n",
      "Rosa Fire\n",
      "Radford Fire\n",
      "Hill Fire\n",
      "Fairview Fire\n",
      "Caesar Fire\n",
      "Sandia Fire\n",
      "Red Fire\n",
      "Tower Fire\n",
      "Mountain Fire\n",
      "Mill Fire\n",
      "Woods Fire\n",
      "Walker Fire\n",
      "Branstetter Fire\n",
      "Border 32\n",
      "Route Fire\n",
      "McCovey Fire\n",
      "Gulch Fire\n",
      "Ranch Fire\n",
      "Still Fire\n",
      "East Fire\n",
      "Valley Fire\n",
      "Point Fire\n",
      "Pleasant Fire\n",
      "Quail Fire\n",
      "Los Bueyes\n",
      "Wishon Fire\n",
      "Eden Fire\n",
      "Oak Fire\n",
      "Rail Fire\n",
      "Sam Fire\n",
      "S-2 Fire\n",
      "Rodgers Fire\n",
      "3-8 Fire\n",
      "Six Rivers (SRF) Lightning Complex\n",
      "Smokey Fire\n",
      "Springs Fire\n",
      "Marmot Fire\n",
      "Meamber Fire\n",
      "Pebble Fire\n",
      "Kelsey Fire\n",
      "Shackleford Fire\n",
      "Mesa Fire\n",
      "Yeti and Alex Fire\n",
      "Highway Fire\n",
      "McKinney Fire\n",
      "Apple Fire\n",
      "Casner Fire\n",
      "Cable Fire\n",
      "Oak Fire\n",
      "Anzar Fire\n",
      "Slate Fire\n",
      "Flynn Fire\n",
      "Meadow Fire\n",
      "Winding Fire\n",
      "Agua Fire\n",
      "Bell Fire\n",
      "Riosa Fire\n",
      "Border 27 Fire\n",
      "Rainbow Fire\n",
      "Grant Fire\n",
      "Peter Fire\n",
      "Herman Fire\n",
      "Harbison\n",
      "Porter Fire\n",
      "Bay Fire\n",
      "Washburn Fire\n",
      "Nome Fire\n",
      "Jan-Dar Fire\n",
      "Electra Fire\n",
      "Garrison Fire\n",
      "Table Fire\n",
      "Sandra Fire\n",
      "Nelson Fire\n",
      "Evan Hewes\n",
      "Rices Fire\n",
      "Burrows Fire\n",
      "Camino Fire\n",
      "Redwood Fire\n",
      "Roblar Fire\n",
      "Ridge Fire\n",
      "Union Fire\n",
      "Kirker Fire\n",
      "Romero Fire\n",
      "Tesla Fire\n",
      "Scenic Fire\n",
      "Timm Fire\n",
      "Canyon Fire\n",
      "Thunder Fire\n",
      "99 Fire\n",
      "Edgewood\n",
      "Border 16\n",
      "Rancho Fire\n",
      "Border 13 Fire\n",
      "Barrett Fire\n",
      "Sheep Fire\n",
      "Brandie Fire\n",
      "Plant Fire\n",
      "Graham Fire\n",
      "Park Fire\n",
      "Marsh Fire\n",
      "Eagle Fire\n",
      "Hesperia Fire\n",
      "62 Fire\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m                \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39min fhsz\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[17], line 38\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m read_shapefiles()\n\u001b[0;32m     31\u001b[0m \u001b[39m#blank_dict = {key: [] for key in CSV_COLUMN_NAMES}\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m#new_fire_df = pd.DataFrame.from_dict(blank_dict)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39m#new_fire_df.to_csv('fire_data.csv', header=CSV_COLUMN_NAMES, index=False)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39m#wait an hour between webscraping checks\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39m#time.sleep(HOUR_IN_SECONDS)\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m new_incidents_available, new_fire_names \u001b[39m=\u001b[39m webscraper\u001b[39m.\u001b[39;49mupdate_data_file()\n\u001b[0;32m     39\u001b[0m \u001b[39mif\u001b[39;00m new_incidents_available \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     twitter_bot\u001b[39m.\u001b[39mtweet(compile_tweet_text(new_fire_names))\n",
      "File \u001b[1;32mc:\\Users\\Isaac\\Documents\\GitHub\\wherefire-python\\webscraper.py:64\u001b[0m, in \u001b[0;36mupdate_data_file\u001b[1;34m()\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39m#TODO:: use forking to make the following faster\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mwith\u001b[39;00m requests\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m s:\n\u001b[1;32m---> 64\u001b[0m     response \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39;49mget(incident_url, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[0;32m     65\u001b[0m     response_xml \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mtext\n\u001b[0;32m     66\u001b[0m     soup \u001b[39m=\u001b[39m BS(response_xml, \u001b[39m'\u001b[39m\u001b[39mlxml\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\requests\\sessions.py:600\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    593\u001b[0m \n\u001b[0;32m    594\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \u001b[39m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    596\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    599\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 600\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest(\u001b[39m\"\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m\"\u001b[39m, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    500\u001b[0m         )\n\u001b[0;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1373\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1375\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1270\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1271\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1272\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1273\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1274\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1275\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1128\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1129\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1131\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#webscraper is webscraper.py\n",
    "import webscraper\n",
    "#twitter_bot is twitter_bot.py\n",
    "import twitter_bot\n",
    "import time\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "#globals\n",
    "#=======\n",
    "\n",
    "gdf_list = [] #a list of all geodataframes\n",
    "\n",
    "#=======\n",
    "#globals\n",
    "\n",
    "#constants\n",
    "#=========\n",
    "\n",
    "HOUR_IN_SECONDS = 86400\n",
    "CSV_COLUMN_NAMES = ['name','county','date_started','latitude','longitude']\n",
    "SHAPEFILE_NAMES = ['./shapefiles/full_california_fhsz/fhszs06_3.shp']\n",
    "\n",
    "#\\constants\n",
    "#==========\n",
    "\n",
    "\n",
    "def main():\n",
    "        read_shapefiles()\n",
    "        #blank_dict = {key: [] for key in CSV_COLUMN_NAMES}\n",
    "        #new_fire_df = pd.DataFrame.from_dict(blank_dict)\n",
    "        #new_fire_df.to_csv('fire_data.csv', header=CSV_COLUMN_NAMES, index=False)\n",
    "    #main server loop\n",
    "    #while(1):\n",
    "        #wait an hour between webscraping checks\n",
    "        #time.sleep(HOUR_IN_SECONDS)\n",
    "        new_incidents_available, new_fire_names = webscraper.update_data_file()\n",
    "        if new_incidents_available == True:\n",
    "            twitter_bot.tweet(compile_tweet_text(new_fire_names))\n",
    "\n",
    "def compile_tweet_text(new_fire_names):\n",
    "    tweet_text_list = []\n",
    "    df = pd.read_csv('fire_data.csv')\n",
    "    #print(df)\n",
    "    search_column_name = CSV_COLUMN_NAMES[0]\n",
    "    for new_fire_name in new_fire_names:\n",
    "        fire_result = df[df[search_column_name] == new_fire_name]\n",
    "        county_name = fire_result[CSV_COLUMN_NAMES[1]]\n",
    "        date_started = fire_result[CSV_COLUMN_NAMES[2]]\n",
    "        latitude = fire_result[CSV_COLUMN_NAMES[3]]\n",
    "        longitude = fire_result[CSV_COLUMN_NAMES[4]]\n",
    "        fhsz_text = \"\"\n",
    "        if fhsz(latitude, longitude):\n",
    "            fhsz_text = \"The fire is in an area designated as a fire hazard zone.\"\n",
    "        tweet_text = \"ALERT\\n\\nNew fire: \" + new_fire_name + \"\\nCounty: \" + county_name + \"\\nStarted on: \" + date_started + \"\\n\" + fhsz_text\n",
    "        tweet_text_list.append(tweet_text)\n",
    "    return tweet_text_list\n",
    "\n",
    "#constructs fhsz polygons from shapefiles\n",
    "def read_shapefiles():\n",
    "    for shapefile_name in SHAPEFILE_NAMES:\n",
    "        gdf = gpd.read_file(shapefile_name) #gdf is geodataframe\n",
    "        gdf_list.append(gdf)        \n",
    "\n",
    "#returns what classification of fire hazard severity zone a location is in (none,)\n",
    "def fhsz(lat,long):\n",
    "   point = Point(lat,long)\n",
    "   for gdf in gdf_list:\n",
    "       for polygon in gdf.geometry:\n",
    "           if point.within(polygon):\n",
    "               print('in fhsz')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
